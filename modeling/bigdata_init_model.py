# -*- coding: utf-8 -*-
"""bigdata_init_model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15gM4uXlMhfqmcY-TZjgLgFWQcSaeyaPB

### Import Dependencies
"""

pip install pyspark

# Data wrangling
import pandas as pd 

# Visualization
import matplotlib.pyplot as plt 
import seaborn as sns 

# Date wrangling
import datetime
import os

# Math operations
import numpy as np

# Random sampling
import random

# sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline

# Keras API 
import tensorflow as tf
from tensorflow import keras

# Deep learning 
from keras.models import Input, Model, Sequential
from keras.layers import Dense, Dropout, LSTM, Concatenate, SimpleRNN, Masking, Flatten
from keras import losses
from keras.callbacks import EarlyStopping
from keras.initializers import RandomNormal

# pyspark related
# pip install pyspark
import pyspark
from pyspark.sql.functions import split, explode, to_timestamp, from_unixtime, from_utc_timestamp
from pyspark import SparkConf, SparkContext,SQLContext
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,ArrayType,StringType,DoubleType
from pyspark.sql.functions import *
from pyspark.sql import Window
# from pyspark.ml.feature import StandardScaler
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
#from ts.flint import FlintContext
sc = SparkContext.getOrCreate()
sqlContext = SQLContext(sc)

"""### Data

#### Get Historic Data
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# if [ ! -d '/content/data' ]; then
#   git clone https://gitlab.com/Dimu_1020/big_data_final '/content/data'
#   cd '/content/data'
# else
#   echo "Dataset already downloaded in '/content/data'"
# fi

spark = SparkSession.builder.appName("DataFrame").getOrCreate()
path_to_file = 'data/HistoryBulk_NY.csv'
df = spark.read.csv(path_to_file, header=True)
df.show()

"""#### Pre-processing"""

# convert to datestamp
spark.conf.set('spark.sql.session.timeZone', 'UTC')
df = df.withColumn("dt",from_unixtime(unix_timestamp(col("dt_iso"),"yyyy-MM-dd HH:mm:ss '+0000 UTC'"),'yyyy-MM-dd HH:mm:ss').cast("timestamp"))
# Sorting by the date 
df = df.sort(asc("dt"))
df.show()

# Listing the min and the max dates
first = df.agg({'dt': "min"}).collect()[0][0] 
last = df.agg({'dt': "max"}).collect()[0][0]
print(f"First date {first}")
print(f"Most recent date {last}")

df = df.withColumn("date",to_date("dt_iso"))
df.show()

# Aggregating to hourly level
features = ['temp', 'pressure', 'wind_speed','Weather_main']
data_col = ['dt', 'date'] + features

df_data = df.select(data_col).withColumn("temp", avg("temp").over(Window.partitionBy("dt"))) \
                   .withColumn("pressure", avg("pressure").over(Window.partitionBy("dt"))) \
                   .withColumn("wind_speed", avg("wind_speed").over(Window.partitionBy("dt")))
df_data.show()

"""### Feature Selection"""

# get integer timestamp
df_data = df_data.withColumn('timestamp',unix_timestamp(col('dt'), format='yyyy-MM-dd HH:mm:ss').alias('unix_timestamp'))

# Seconds in day 
SECOND = 24 * 60 * 60 

# Seconds in year 
YEAR = (365.25) * SECOND

features_final = ['temp', 'day_cos', 'day_sin', 'month_sin', 'month_cos', 'pressure', 'wind_speed', 'weather_main']
df_final = df_data.withColumn('hour', hour(df_data.dt)) \
       .withColumn('month', month(df_data.dt)) \
       .withColumn('day_cos', cos(col('hour') * (2 * np.pi / 24))) \
       .withColumn('day_sin', sin(col('hour') * (2 * np.pi /24))) \
       .withColumn('month_cos', cos(col('timestamp') * (2 * np.pi /YEAR))) \
       .withColumn('month_sin', sin(col('timestamp') * (2 * np.pi /YEAR))) \
       .select(features_final)
df_final.show()

pd_final = df_final.toPandas()
sunny = pd_final.weather_main == 'Clear'
cloudy = pd_final.weather_main == 'Clouds'
windy = pd_final.weather_main.isin(['Squall',  'Tornado'])
foggy =  pd_final.weather_main.isin(['Mist', 'Haze', 'Dust', 'Fog', 'Smoke'])
rainy = pd_final.weather_main.isin(['Drizzle', 'Rain', 'Thunderstorm'])
snowy = pd_final.weather_main .isin(['Snow'])
pd_final['weather_main'] = pd_final['weather_main'].where(~sunny, 'sunny')
pd_final['weather_main'] = pd_final['weather_main'].where(~cloudy, 'cloudy')
pd_final['weather_main'] = pd_final['weather_main'].where(~windy, 'windy')
pd_final['weather_main'] = pd_final['weather_main'].where(~foggy, 'foggy')
pd_final['weather_main'] = pd_final['weather_main'].where(~rainy, 'rainy')
pd_final['weather_main'] = pd_final['weather_main'].where(~snowy, 'snowy')
conditions = [
    (pd_final['weather_main'] == 'sunny'),
    (pd_final['weather_main'] == 'cloudy'),
    (pd_final['weather_main'] == 'windy'),
    (pd_final['weather_main'] == 'foggy'),
    (pd_final['weather_main'] == 'rainy'),
    (pd_final['weather_main'] == 'snowy')
]
values = [0, 1, 2, 3, 4, 5]
pd_final['weather_id'] = np.select(conditions, values)

pd_final

if not os.path.exists("/content/data/pd_final.pkl"):
    pd_final.to_pickle('/content/data/pd_final.pkl')
else:
    pd_final = pd.read_pickle("/content/data/pd_final.pkl")
pd_final

"""#### Train model"""

# Share of obs in testing 
train_test_split_share = 0.1

# Number of lags (hours back) to use for models
lag = 48

# Steps ahead to forecast 
n_ahead = 1

scaler = StandardScaler()
np_final = scaler.fit_transform(pd_final[['temp', 'day_cos', 'day_sin', 'month_sin', 'month_cos', 'pressure',
       'wind_speed']])
# np_final.concat(pd_final['weather_id'])
np_final = np.append(np_final, pd_final['weather_id'].values.reshape(-1,1).astype('int'), axis=1)
np_final

"""#### Convert to time series dataset"""

def create_X_Y(ts: np.array, _lag=48, n_ahead=1, target_index=0) -> tuple:
    """
    - ts: A time series dataframe in series form
    - lag: Number of lags (hours back) to use for models
    - n_ahead: Steps ahead to forecast 
    Using <lag> number of data: x_{i}, x_{i+1}, ...x_{i+lag-1} to predict <n_ahead>
    number of data x_{i+lag}, ..., x_{i+lag+n_ahead-1}
    Thus, X = x_{i}, x_{i+1}, ...x_{i+lag-1}
        , Y = x_{i+lag}, ..., x_{i+lag+n_ahead-1}
    """
    # Extract # of features and # of observations
    n_obs = ts.shape[0]
    n_features = ts.shape[1]
    
    # Creating placeholder lists
    X, Y = [], []

    # if we don't have enough obs to predict Y
    if n_obs - _lag <= 0:
        X.append(ts) # no label, only x
    else:
        for i in range(n_obs - _lag - n_ahead):
            Y.append(ts[(i + _lag):(i + _lag + n_ahead), target_index])
            X.append(ts[i:(i + _lag)])

    X, Y = np.array(X), np.array(Y)

    # Reshaping the X array to an RNN input shape 
    X = np.reshape(X, (X.shape[0], _lag, n_features))

    return X, Y
X, Y = create_X_Y(np_final, 48, n_ahead=n_ahead, target_index = 7)

# # Subseting only the needed columns 
# ts = pd_final

# nrows = ts.shape[0]

# # Spliting into train and test sets
# test_share = 0.25
# train = ts[0:int(nrows * (1 - test_share))]
# test = ts[int(nrows * (1 - test_share)):]

# # Creating the final scaled frame 
# ts_s = pd.concat([train, test])
# X, Y = create_X_Y(ts_s.values,n_ahead=n_ahead)

n_ft = X.shape[2]

# Spliting into train and test sets 
Xtrain, Ytrain = X[0:int(X.shape[0] * (1 - 0.25))], Y[0:int(X.shape[0] * (1 - 0.25))]
X_val, Y_val = X[int(X.shape[0] * (1 - 0.25)):], Y[int(X.shape[0] * (1 - 0.25)):]
Ytrain = Ytrain.astype('int')
Y_val= Y_val.astype('int')

"""#### LSTM"""

class NNMultistepModel():
    
    def __init__(
        self, 
        X, 
        Y, 
        n_outputs,
        n_lag,
        n_ft,
        n_layer,
        batch,
        epochs, 
        lr,
        Xval=None,
        Yval=None,
        mask_value=-999.0,
        min_delta=0.001,
        patience=5
    ):
        lstm_input = Input(shape=(n_lag, n_ft))

        # Series signal 
        lstm_layer = LSTM(n_layer, activation='relu')(lstm_input)

        x = Dense(n_outputs)(lstm_layer)
        
        self.model = Model(inputs=lstm_input, outputs=x)
        self.batch = batch 
        self.epochs = epochs
        self.n_layer=n_layer
        self.lr = lr 
        self.Xval = Xval
        self.Yval = Yval
        self.X = X
        self.Y = Y
        self.mask_value = mask_value
        self.min_delta = min_delta
        self.patience = patience

    def trainCallback(self):
        return EarlyStopping(monitor='loss', patience=self.patience, min_delta=self.min_delta)

    def train(self):
        # Getting the untrained model 
        empty_model = self.model
        
        # Initiating the optimizer
        optimizer = keras.optimizers.Adam(learning_rate=self.lr)

        # Compiling the model
        empty_model.compile(optimizer=optimizer,
                            loss=losses.MeanAbsoluteError(), 
                            # loss=tf.keras.losses.CategoricalCrossentropy(),
                            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

        if (self.Xval is not None) & (self.Yval is not None):
            history = empty_model.fit(
                self.X, 
                self.Y, 
                epochs=self.epochs, 
                batch_size=self.batch, 
                validation_data=(self.Xval, self.Yval), 
                shuffle=False,
                callbacks=[self.trainCallback()]
            )
        else:
            history = empty_model.fit(
                self.X, 
                self.Y, 
                epochs=self.epochs, 
                batch_size=self.batch,
                shuffle=False,
                callbacks=[self.trainCallback()]
            )
        
        # Saving to original model attribute in the class
        self.model = empty_model
        
        # Returning the training history
        return history
    
    def predict(self, X):
        return self.model.predict(X)

model = NNMultistepModel(
    X=Xtrain,
    Y=Ytrain,
    n_outputs=n_ahead,
    n_lag=lag,
    n_ft=n_ft,
    n_layer=10,
    batch=512,
    epochs=20, 
    lr=0.001,
    Xval=X_val,
    Yval=Y_val,
)

model.model.summary()

history = model.train()

sns.set()
loss = history.history.get('loss')
val_loss = history.history.get('val_loss')
n_epochs = range(len(loss))

fig = plt.figure(figsize = (8, 5))
plt.plot(loss, '.-', 
         label='train_loss', color = 'darkblue')
plt.plot(val_loss, '.-',
         label = 'val_loss', color = 'darkred')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(np.arange(0, 20, 1))
plt.legend(loc='upper right')
plt.title('Loss of LSTM model')

# import pickle
# filename = 'NNMultistepModel_lstm_model.sav'
# pickle.dump(model, open(filename, 'wb'))

# # some time later...
# # load the model from disk
# loaded_model = pickle.load(open(filename, 'rb'))
# result = model.predict(X_val)
# print(np.around(result).astype(np.uint64))

model.model.save("NNMultistepModel_lstm_model.h5")

from google.colab import files
files.download("/content/NNMultistepModel_lstm_model.h5")